# OllamaMax - AI Chat Assistant

<p align="center">
  <img src="./static/images/ollama.png" alt="Ollama Logo" width="200">
</p>

> A modern, fast, and lightweight web interface for interacting with Ollama's large language models

## ğŸš€ Quick Links

- **[Complete Documentation](./docs/README.md)** - Full setup guide, features, and model library
- **[Code Execution Flow](./docs/CODE_EXECUTION_ANALYSIS.md)** - Detailed line-by-line code analysis
- **[Mermaid Diagrams](./docs/MERMAID_FLOW_DIAGRAMS.md)** - Visual flow charts and sequence diagrams
- **[Model Pull System](./docs/MODEL_PULL_SYSTEM.md)** - Auto-download and optimization
- **[Development Notes](./docs/)** - Feature docs and TODOs

## âœ¨ Features

- ğŸ¨ **Modern Dark Theme UI** - ChatGPT-inspired interface
- ğŸ”„ **Real-time Communication** - WebSocket support with auto-reconnect
- ğŸ¤– **Multi-Model Support** - Switch between coding, vision, and chat models
- ğŸ“¦ **Auto-Download Models** - Automatic model pulling when needed
- ğŸ’» **Code Syntax Highlighting** - VSCode-style highlighting with copy buttons
- ğŸ“ **Markdown Rendering** - Full markdown support for rich responses
- âš¡ **Zero Dependencies Frontend** - No Node.js, no frameworks, pure HTML/CSS/JS

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| **Backend** | Go 1.21+ with Gorilla Mux & WebSockets |
| **Frontend** | Vanilla JavaScript + marked.js + highlight.js |
| **LLM Runtime** | Ollama + langchaingo library |
| **Models** | 10+ curated LLMs (coding, vision, chat) |

## ğŸ“¦ Quick Start

### 1. Install Ollama
```bash
# Linux/macOS
curl -fsSL https://ollama.com/install.sh | sh

# Or use the included script
chmod +x ollama_install_basic.sh
./ollama_install_basic.sh
```

### 2. Pull a Model
```bash
ollama pull llama3.1:8b
```

### 3. Build and Run OllamaMax
```bash
# Build the Go backend
go build -o ollamamax main.go

# Run the server
./ollamamax
```

### 4. Open in Browser
Navigate to: `http://localhost:8888`

## ğŸ¯ Supported Models

The application supports **10 carefully selected models** across different use cases:

### Coding & Development
- `qwen2.5-coder:7b` - Code generation and debugging
- `deepseek-coder:6.7b` - Algorithm and problem-solving
- `deepseek-r1` - Reasoning-focused coding
- `glm-4.6` - Multilingual code support

### Vision & Multimodal
- `qwen3-vl` - OCR, charts, diagram analysis
- `llava:7b` - Image understanding

### General Chat
- `llama3.1:8b` - Meta's balanced all-rounder (default)
- `qwen3:7b` - Long-context chat
- `gemma2:9b` - Fast and efficient
- `mistral:7b` - General purpose

### Embeddings
- `nomic-embed-text` - For RAG pipelines

**System Requirements:**
- 8GB RAM minimum for 7B models
- 16GB RAM recommended for multiple models
- GPU optional but recommended (CUDA/ROCm)

## ğŸ“Š Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      WebSocket/HTTP      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser   â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚  Go Server   â”‚
â”‚   (JS/HTML) â”‚                           â”‚  :8888       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                                                 â”‚ langchaingo
                                                 â–¼
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚    Ollama    â”‚
                                          â”‚   Service    â”‚
                                          â”‚  :11434      â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Components:**
1. **Frontend** - Model selector, chat UI, markdown renderer
2. **Go Backend** - WebSocket/HTTP handlers, model management
3. **Ollama Service** - LLM inference engine

**Data Flow:**
```
User Selects Model â†’ JS captures selection â†’ WebSocket sends JSON â†’
Go validates model â†’ Auto-pulls if missing â†’ Updates currentModel â†’
Sends query to Ollama â†’ Receives response â†’ Renders in UI
```

See [CODE_EXECUTION_ANALYSIS.md](./docs/CODE_EXECUTION_ANALYSIS.md) for complete execution flow with line numbers.

## ğŸ“– Documentation Structure

```
docs/
â”œâ”€â”€ README.md                      # Full user guide and setup
â”œâ”€â”€ CODE_EXECUTION_ANALYSIS.md     # Line-by-line code walkthrough
â”œâ”€â”€ MERMAID_FLOW_DIAGRAMS.md       # Visual flow charts
â”œâ”€â”€ MODEL_PULL_SYSTEM.md           # Auto-download system
â”œâ”€â”€ MODELS_ALIGNMENT.md            # Model selection UI
â”œâ”€â”€ BOTTOM_INPUT_FEATURE.md        # Chat input implementation
â”œâ”€â”€ UI_SPACING_IMPROVEMENTS.md     # UI/UX enhancements
â”œâ”€â”€ TODO.md                        # Roadmap and planned features
â””â”€â”€ WARP.md                        # AI agent instructions
```

## ğŸ”§ Development

### Project Structure
```
.
â”œâ”€â”€ main.go                    # Go backend server
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ index.html            # Main UI
â”‚   â”œâ”€â”€ script.js             # Frontend logic
â”‚   â”œâ”€â”€ styles.css            # Dark theme styling
â”‚   â””â”€â”€ images/               # Logo assets
â”œâ”€â”€ ollama_pull_and_run.sh    # Model optimization script
â”œâ”€â”€ ollama_install_basic.sh   # Ollama installer
â””â”€â”€ docs/                     # Documentation
```

### Key Files
- **main.go** (589 lines)
  - Lines 47-67: Available models array
  - Lines 127-198: HTTP router setup
  - Lines 308-404: WebSocket handler
  - Lines 451-476: Model installation check
  - Lines 479-515: Model auto-pull
  - Lines 563-588: LLM query processing

- **script.js** (589 lines)
  - Lines 24-72: WebSocket connection
  - Lines 147-268: Message rendering
  - Lines 336-381: Send message logic
  - Lines 454-484: Model selection handler

### Building
```bash
# Development build
go build -o ollamamax main.go

# Production build (optimized)
go build -ldflags="-s -w" -o ollamamax main.go
```

### Testing
```bash
# Check Ollama status
curl http://localhost:8888/api/health

# Get available models
curl http://localhost:8888/api/models

# Test chat (HTTP)
curl -X POST http://localhost:8888/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message":"Hello","model_name":"llama3.1:8b"}'
```

## ğŸ¨ Features in Detail

### Auto-Pull Models
When you select a model that isn't installed, OllamaMax automatically:
1. Detects the missing model via `ollama list`
2. Shows a download progress indicator
3. Pulls the model using `ollama_pull_and_run.sh` (optimized)
4. Falls back to direct `ollama pull` if script fails
5. Updates the UI when ready

### WebSocket Communication
- Real-time bidirectional communication
- Auto-reconnect on connection loss
- Typing indicators during inference
- Fallback to HTTP if WebSocket unavailable

### Code Highlighting
- 30+ languages supported via highlight.js
- Copy buttons for individual code blocks
- Copy entire message button
- Language badges on code blocks

## ğŸ¤ Contributing

This is a personal project, but suggestions are welcome! See [docs/TODO.md](./docs/TODO.md) for planned features.

## ğŸ“ License

This project uses Ollama, which is licensed under the MIT License.

## ğŸ”— Related Links

- [Ollama Official Site](https://ollama.com/)
- [Ollama GitHub](https://github.com/ollama/ollama)
- [Ollama Model Library](https://ollama.com/library)
- [langchaingo Documentation](https://github.com/tmc/langchaingo)

## ğŸ“Š Recent Changes

### November 22, 2024
- âœ… Organized all documentation into `docs/` folder
- âœ… Created comprehensive code execution flow analysis
- âœ… Added Mermaid flow diagrams (7 different visualizations)
- âœ… Documented complete architecture and data flow
- âœ… Updated README with clear navigation structure

---

**Made with â¤ï¸ using Go, Ollama, and zero frontend frameworks**
